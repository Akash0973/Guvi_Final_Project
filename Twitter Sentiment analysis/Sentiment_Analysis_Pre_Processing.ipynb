{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pd_59s01ivb1"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns=['Target','IDs','Date','Flag','User','Text']\n",
        "\n",
        "with open(\"twitter_new.csv\",\n",
        "          'r', encoding='utf-8', errors='ignore') as file:\n",
        "    data=pd.read_csv(file, header=None, names=columns)\n",
        "\n",
        "data['Weekday']=data['Date'].apply(lambda x: x.split()[0])\n",
        "data['Month']=data['Date'].apply(lambda x: x.split()[1])\n",
        "data['Day']=data['Date'].apply(lambda x: x.split()[2])\n",
        "data['Time']=data['Date'].apply(lambda x: x.split()[3])\n",
        "data['Timezone']=data['Date'].apply(lambda x: x.split()[4])\n",
        "data['Year']=data['Date'].apply(lambda x: x.split()[5])\n",
        "\n",
        "data=data.drop('Date', axis=1)\n",
        "\n",
        "#Find total unique values\n",
        "cols=data.columns\n",
        "for col in cols:\n",
        "    print('Total Unique ['+ col + '] = ' + str(len(data[col].unique())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uELsgTXdx-kv",
        "outputId": "acd97d2c-e7d9-4b76-fda5-de0a12cde9be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Unique [Target] = 2\n",
            "Total Unique [IDs] = 1598315\n",
            "Total Unique [Flag] = 1\n",
            "Total Unique [User] = 659775\n",
            "Total Unique [Text] = 1581466\n",
            "Total Unique [Weekday] = 7\n",
            "Total Unique [Month] = 3\n",
            "Total Unique [Day] = 29\n",
            "Total Unique [Time] = 86386\n",
            "Total Unique [Timezone] = 1\n",
            "Total Unique [Year] = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Below three columns only have 1 unique value\n",
        "data=data.drop('Flag', axis=1)\n",
        "data=data.drop('Timezone', axis=1)\n",
        "data=data.drop('Year', axis=1)\n",
        "\n",
        "#Convert time to seconds\n",
        "def time_to_sec(time):\n",
        "    H,M,S=time.split(':')\n",
        "    return (int(H)*3600+int(M)*60+int(S))\n",
        "\n",
        "data['Time']=data['Time'].apply(time_to_sec)\n",
        "data['Day']=data['Day'].apply(lambda x: int(x))\n",
        "\n",
        "#User and User IDs are not relevant to sentiment analysis so we drop them\n",
        "data=data.drop('User', axis=1)\n",
        "data=data.drop('IDs', axis=1)\n",
        "\n",
        "#Find total unique values\n",
        "cols=data.columns\n",
        "for col in cols:\n",
        "    print('Total Unique ['+ col + '] = ' + str(len(data[col].unique())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97sezBGdv3Ng",
        "outputId": "fc455baa-2de4-4e41-8b4b-060d6102dc50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Unique [Target] = 2\n",
            "Total Unique [Text] = 1581466\n",
            "Total Unique [Weekday] = 7\n",
            "Total Unique [Month] = 3\n",
            "Total Unique [Day] = 29\n",
            "Total Unique [Time] = 86386\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we clean the text and process it\n",
        "def text_cleaning(input_string):\n",
        "    pattern = r'https?://\\S+|www\\.\\S+|\\S+\\.([a-z]{2,})\\b'\n",
        "    cleaned_string = re.sub(pattern, '', input_string)\n",
        "    cleaned_string = re.sub(r'[^\\w\\s]', '', cleaned_string)\n",
        "    cleaned_string = re.sub(r'\\s+', ' ', cleaned_string)\n",
        "    pattern = r'\\b\\d+\\b'\n",
        "    cleaned_string = re.sub(pattern, '', cleaned_string)\n",
        "    cleaned_string = re.sub(r'\\d+', '', cleaned_string)\n",
        "    return cleaned_string.strip()\n",
        "\n",
        "data['Text']=data['Text'].apply(text_cleaning)\n",
        "\n",
        "#Remove all the stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords_and_stemming(input_string):\n",
        "    words=nltk.word_tokenize(input_string)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    cleaned_string = ' '.join(words)\n",
        "    return cleaned_string\n",
        "\n",
        "data['Text']=data['Text'].apply(remove_stopwords_and_stemming)\n",
        "data=data[data['Text']!='']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaHufUDl5O6_",
        "outputId": "f16ccc0f-4396-496a-f2fd-b10360dc5232"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxelatXv7i0n",
        "outputId": "98174237-81ba-44f0-f443-48009c03e0de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1599098 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column   Non-Null Count    Dtype \n",
            "---  ------   --------------    ----- \n",
            " 0   Target   1599098 non-null  int64 \n",
            " 1   Text     1599098 non-null  object\n",
            " 2   Weekday  1599098 non-null  object\n",
            " 3   Month    1599098 non-null  object\n",
            " 4   Day      1599098 non-null  int64 \n",
            " 5   Time     1599098 non-null  int64 \n",
            "dtypes: int64(3), object(3)\n",
            "memory usage: 85.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('Cleaned_Data.csv', index=False)"
      ],
      "metadata": {
        "id": "F_acy4Jb7keE"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}