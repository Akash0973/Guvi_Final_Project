{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aeafe46-cdd1-41fc-ae39-e3768b0cf831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c3ca29-ccff-47c7-89c1-4d91556670d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5622f0-bfa2-4c0b-9f26-0683b518a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=r'C:\\Guvi_final_project\\Image Classification\\dataset\\training_set', transform=transform)\n",
    "train_loader = Data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root=r'C:\\Guvi_final_project\\Image Classification\\dataset\\test_set', transform=transform)\n",
    "test_loader = Data.DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1e22855-d788-4501-a4bf-af621a16cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.convres = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pool(self.pool(self.pool(self.convres(x))))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x += y\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4ae08e-621a-462b-bc7b-f8533ef0c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),momentum=0.9,lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872eef00-ca28-4720-a633-942696c75449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step [500/2000], Loss: 0.4519\n",
      "Epoch 1/30, Step [1000/2000], Loss: 0.5642\n",
      "Epoch 1/30, Step [1500/2000], Loss: 0.5030\n",
      "Epoch 1/30, Step [2000/2000], Loss: 0.3807\n",
      "Accuracy on the test images: 66.25%\n",
      "Accuracy on the train images: 65.8375%\n",
      "Epoch 2/30, Step [500/2000], Loss: 0.6324\n",
      "Epoch 2/30, Step [1000/2000], Loss: 0.4885\n",
      "Epoch 2/30, Step [1500/2000], Loss: 0.9057\n",
      "Epoch 2/30, Step [2000/2000], Loss: 0.6156\n",
      "Accuracy on the test images: 69.4%\n",
      "Accuracy on the train images: 71.2375%\n",
      "Epoch 3/30, Step [500/2000], Loss: 0.6981\n",
      "Epoch 3/30, Step [1000/2000], Loss: 0.6236\n",
      "Epoch 3/30, Step [1500/2000], Loss: 0.7666\n",
      "Epoch 3/30, Step [2000/2000], Loss: 0.5796\n",
      "Accuracy on the test images: 72.75%\n",
      "Accuracy on the train images: 72.5125%\n",
      "Epoch 4/30, Step [500/2000], Loss: 0.5756\n",
      "Epoch 4/30, Step [1000/2000], Loss: 0.9157\n",
      "Epoch 4/30, Step [1500/2000], Loss: 0.5459\n",
      "Epoch 4/30, Step [2000/2000], Loss: 0.7777\n",
      "Accuracy on the test images: 73.2%\n",
      "Accuracy on the train images: 74.275%\n",
      "Epoch 5/30, Step [500/2000], Loss: 0.7276\n",
      "Epoch 5/30, Step [1000/2000], Loss: 0.8926\n",
      "Epoch 5/30, Step [1500/2000], Loss: 1.2713\n",
      "Epoch 5/30, Step [2000/2000], Loss: 0.1352\n",
      "Accuracy on the test images: 73.65%\n",
      "Accuracy on the train images: 74.7625%\n",
      "Epoch 6/30, Step [500/2000], Loss: 0.5048\n",
      "Epoch 6/30, Step [1000/2000], Loss: 0.5117\n",
      "Epoch 6/30, Step [1500/2000], Loss: 0.6035\n",
      "Epoch 6/30, Step [2000/2000], Loss: 0.1213\n",
      "Accuracy on the test images: 73.4%\n",
      "Accuracy on the train images: 74.375%\n",
      "Epoch 7/30, Step [500/2000], Loss: 0.4094\n",
      "Epoch 7/30, Step [1000/2000], Loss: 0.3159\n",
      "Epoch 7/30, Step [1500/2000], Loss: 0.9242\n",
      "Epoch 7/30, Step [2000/2000], Loss: 0.4308\n",
      "Accuracy on the test images: 74.1%\n",
      "Accuracy on the train images: 76.525%\n",
      "Epoch 8/30, Step [500/2000], Loss: 0.0969\n",
      "Epoch 8/30, Step [1000/2000], Loss: 0.2511\n",
      "Epoch 8/30, Step [1500/2000], Loss: 0.2908\n",
      "Epoch 8/30, Step [2000/2000], Loss: 0.9073\n",
      "Accuracy on the test images: 77.6%\n",
      "Accuracy on the train images: 78.075%\n",
      "Epoch 9/30, Step [500/2000], Loss: 0.6300\n",
      "Epoch 9/30, Step [1000/2000], Loss: 0.4385\n",
      "Epoch 9/30, Step [1500/2000], Loss: 0.6159\n",
      "Epoch 9/30, Step [2000/2000], Loss: 0.4244\n",
      "Accuracy on the test images: 73.05%\n",
      "Accuracy on the train images: 75.925%\n",
      "Epoch 10/30, Step [500/2000], Loss: 0.5741\n",
      "Epoch 10/30, Step [1000/2000], Loss: 0.1625\n",
      "Epoch 10/30, Step [1500/2000], Loss: 0.3517\n",
      "Epoch 10/30, Step [2000/2000], Loss: 0.4198\n",
      "Accuracy on the test images: 77.45%\n",
      "Accuracy on the train images: 79.075%\n",
      "Epoch 11/30, Step [500/2000], Loss: 0.5010\n",
      "Epoch 11/30, Step [1000/2000], Loss: 1.3999\n",
      "Epoch 11/30, Step [1500/2000], Loss: 0.1749\n",
      "Epoch 11/30, Step [2000/2000], Loss: 0.1614\n",
      "Accuracy on the test images: 78.45%\n",
      "Accuracy on the train images: 80.3125%\n",
      "Epoch 12/30, Step [500/2000], Loss: 0.6503\n",
      "Epoch 12/30, Step [1000/2000], Loss: 0.1838\n",
      "Epoch 12/30, Step [1500/2000], Loss: 0.4595\n",
      "Epoch 12/30, Step [2000/2000], Loss: 0.5020\n",
      "Accuracy on the test images: 76.95%\n",
      "Accuracy on the train images: 78.325%\n",
      "Epoch 13/30, Step [500/2000], Loss: 1.1013\n",
      "Epoch 13/30, Step [1000/2000], Loss: 0.1834\n",
      "Epoch 13/30, Step [1500/2000], Loss: 0.3859\n",
      "Epoch 13/30, Step [2000/2000], Loss: 0.2719\n",
      "Accuracy on the test images: 78.1%\n",
      "Accuracy on the train images: 80.05%\n",
      "Epoch 14/30, Step [500/2000], Loss: 0.0977\n",
      "Epoch 14/30, Step [1000/2000], Loss: 0.3175\n",
      "Epoch 14/30, Step [1500/2000], Loss: 0.4725\n",
      "Epoch 14/30, Step [2000/2000], Loss: 0.5427\n",
      "Accuracy on the test images: 78.2%\n",
      "Accuracy on the train images: 81.2%\n",
      "Epoch 15/30, Step [500/2000], Loss: 0.3389\n",
      "Epoch 15/30, Step [1000/2000], Loss: 0.4095\n",
      "Epoch 15/30, Step [1500/2000], Loss: 0.6335\n",
      "Epoch 15/30, Step [2000/2000], Loss: 0.4608\n",
      "Accuracy on the test images: 76.75%\n",
      "Accuracy on the train images: 81.4625%\n",
      "Epoch 16/30, Step [500/2000], Loss: 0.2490\n",
      "Epoch 16/30, Step [1000/2000], Loss: 0.1021\n",
      "Epoch 16/30, Step [1500/2000], Loss: 0.4044\n",
      "Epoch 16/30, Step [2000/2000], Loss: 0.6108\n",
      "Accuracy on the test images: 77.55%\n",
      "Accuracy on the train images: 81.675%\n",
      "Epoch 17/30, Step [500/2000], Loss: 0.2145\n",
      "Epoch 17/30, Step [1000/2000], Loss: 0.2450\n",
      "Epoch 17/30, Step [1500/2000], Loss: 0.1892\n",
      "Epoch 17/30, Step [2000/2000], Loss: 1.2131\n",
      "Accuracy on the test images: 76.35%\n",
      "Accuracy on the train images: 80.425%\n",
      "Epoch 18/30, Step [500/2000], Loss: 0.7696\n",
      "Epoch 18/30, Step [1000/2000], Loss: 0.1914\n",
      "Epoch 18/30, Step [1500/2000], Loss: 0.1447\n",
      "Epoch 18/30, Step [2000/2000], Loss: 0.2808\n",
      "Accuracy on the test images: 77.65%\n",
      "Accuracy on the train images: 81.7375%\n",
      "Epoch 19/30, Step [500/2000], Loss: 0.1950\n",
      "Epoch 19/30, Step [1000/2000], Loss: 0.1363\n",
      "Epoch 19/30, Step [1500/2000], Loss: 0.0454\n",
      "Epoch 19/30, Step [2000/2000], Loss: 0.1488\n",
      "Accuracy on the test images: 78.8%\n",
      "Accuracy on the train images: 84.225%\n",
      "Epoch 20/30, Step [500/2000], Loss: 0.0820\n",
      "Epoch 20/30, Step [1000/2000], Loss: 0.2394\n",
      "Epoch 20/30, Step [1500/2000], Loss: 1.7007\n",
      "Epoch 20/30, Step [2000/2000], Loss: 0.1786\n",
      "Accuracy on the test images: 78.1%\n",
      "Accuracy on the train images: 83.5625%\n",
      "Epoch 21/30, Step [500/2000], Loss: 1.0017\n",
      "Epoch 21/30, Step [1000/2000], Loss: 0.5280\n",
      "Epoch 21/30, Step [1500/2000], Loss: 0.4266\n",
      "Epoch 21/30, Step [2000/2000], Loss: 0.2639\n",
      "Accuracy on the test images: 79.6%\n",
      "Accuracy on the train images: 85.5625%\n",
      "Epoch 22/30, Step [500/2000], Loss: 0.1646\n",
      "Epoch 22/30, Step [1000/2000], Loss: 0.3800\n",
      "Epoch 22/30, Step [1500/2000], Loss: 0.2550\n",
      "Epoch 22/30, Step [2000/2000], Loss: 0.1355\n",
      "Accuracy on the test images: 79.2%\n",
      "Accuracy on the train images: 84.4%\n",
      "Epoch 23/30, Step [500/2000], Loss: 1.1374\n",
      "Epoch 23/30, Step [1000/2000], Loss: 0.2833\n",
      "Epoch 23/30, Step [1500/2000], Loss: 0.1189\n",
      "Epoch 23/30, Step [2000/2000], Loss: 0.2366\n",
      "Accuracy on the test images: 79.6%\n",
      "Accuracy on the train images: 85.525%\n",
      "Epoch 24/30, Step [500/2000], Loss: 0.7251\n",
      "Epoch 24/30, Step [1000/2000], Loss: 1.8758\n",
      "Epoch 24/30, Step [1500/2000], Loss: 0.4861\n",
      "Epoch 24/30, Step [2000/2000], Loss: 1.0419\n",
      "Accuracy on the test images: 77.65%\n",
      "Accuracy on the train images: 85.3625%\n",
      "Epoch 25/30, Step [500/2000], Loss: 0.2899\n",
      "Epoch 25/30, Step [1000/2000], Loss: 0.0958\n",
      "Epoch 25/30, Step [1500/2000], Loss: 1.0476\n",
      "Epoch 25/30, Step [2000/2000], Loss: 0.3289\n",
      "Accuracy on the test images: 76.4%\n",
      "Accuracy on the train images: 83.6%\n",
      "Epoch 26/30, Step [500/2000], Loss: 0.1979\n",
      "Epoch 26/30, Step [1000/2000], Loss: 0.2186\n",
      "Epoch 26/30, Step [1500/2000], Loss: 0.3443\n",
      "Epoch 26/30, Step [2000/2000], Loss: 0.1193\n",
      "Accuracy on the test images: 79.9%\n",
      "Accuracy on the train images: 85.95%\n",
      "Epoch 27/30, Step [500/2000], Loss: 0.0885\n",
      "Epoch 27/30, Step [1000/2000], Loss: 0.1190\n",
      "Epoch 27/30, Step [1500/2000], Loss: 0.1242\n",
      "Epoch 27/30, Step [2000/2000], Loss: 0.1554\n",
      "Accuracy on the test images: 79.5%\n",
      "Accuracy on the train images: 86.3125%\n",
      "Epoch 28/30, Step [500/2000], Loss: 0.1733\n",
      "Epoch 28/30, Step [1000/2000], Loss: 0.4085\n",
      "Epoch 28/30, Step [1500/2000], Loss: 0.7497\n",
      "Epoch 28/30, Step [2000/2000], Loss: 0.0301\n",
      "Accuracy on the test images: 78.0%\n",
      "Accuracy on the train images: 86.05%\n",
      "Epoch 29/30, Step [500/2000], Loss: 0.4186\n",
      "Epoch 29/30, Step [1000/2000], Loss: 0.4605\n",
      "Epoch 29/30, Step [1500/2000], Loss: 0.1087\n",
      "Epoch 29/30, Step [2000/2000], Loss: 0.0176\n",
      "Accuracy on the test images: 79.15%\n",
      "Accuracy on the train images: 87.05%\n",
      "Epoch 30/30, Step [500/2000], Loss: 0.3967\n",
      "Epoch 30/30, Step [1000/2000], Loss: 0.1089\n",
      "Epoch 30/30, Step [1500/2000], Loss: 0.9212\n",
      "Epoch 30/30, Step [2000/2000], Loss: 0.1125\n",
      "Accuracy on the test images: 78.1%\n",
      "Accuracy on the train images: 84.4625%\n",
      "Training finished in 15128.442170381546 Seconds!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st=time.time()\n",
    "\n",
    "train_acc0=[]\n",
    "test_acc0=[]\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc0.append(100 * correct / total)\n",
    "    print(f'Accuracy on the test images: {100 * correct / total}%')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc0.append(100 * correct / total)\n",
    "    print(f'Accuracy on the train images: {100 * correct / total}%')\n",
    "\n",
    "    with open('Model3_test.pkl','wb') as file:\n",
    "            pickle.dump(test_acc0,file)\n",
    "    with open('Model3_train.pkl','wb') as file:\n",
    "            pickle.dump(train_acc0,file)\n",
    "    with open('Model3_test.pkl','rb') as file:\n",
    "            test_acc0=pickle.load(file)\n",
    "    with open('Model3_train.pkl','rb') as file:\n",
    "            train_acc0=pickle.load(file)\n",
    "\n",
    "ed=time.time()\n",
    "print('Training finished in '+str(ed-st)+' Seconds!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e34759f0-39ee-43d2-aef2-461a86e3cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.convres = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pool(self.pool(self.pool(self.convres(x))))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x += y\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a1afe4-7822-4824-b266-9ed0667b1c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),momentum=0.9,lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919ca0b9-76f4-4900-9ed8-29e65de9a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step [500/2000], Loss: 0.5384\n",
      "Epoch 1/30, Step [1000/2000], Loss: 0.6657\n",
      "Epoch 1/30, Step [1500/2000], Loss: 0.5247\n",
      "Epoch 1/30, Step [2000/2000], Loss: 0.7046\n",
      "Accuracy on the test images: 59.4%\n",
      "Accuracy on the train images: 60.6625%\n",
      "Epoch 2/30, Step [500/2000], Loss: 0.5271\n",
      "Epoch 2/30, Step [1000/2000], Loss: 0.4468\n",
      "Epoch 2/30, Step [1500/2000], Loss: 0.6901\n",
      "Epoch 2/30, Step [2000/2000], Loss: 0.5911\n",
      "Accuracy on the test images: 68.1%\n",
      "Accuracy on the train images: 67.95%\n",
      "Epoch 3/30, Step [500/2000], Loss: 0.5050\n",
      "Epoch 3/30, Step [1000/2000], Loss: 0.2143\n",
      "Epoch 3/30, Step [1500/2000], Loss: 0.7258\n",
      "Epoch 3/30, Step [2000/2000], Loss: 0.7454\n",
      "Accuracy on the test images: 69.95%\n",
      "Accuracy on the train images: 70.275%\n",
      "Epoch 4/30, Step [500/2000], Loss: 0.6105\n",
      "Epoch 4/30, Step [1000/2000], Loss: 0.2710\n",
      "Epoch 4/30, Step [1500/2000], Loss: 0.5116\n",
      "Epoch 4/30, Step [2000/2000], Loss: 0.4573\n",
      "Accuracy on the test images: 71.1%\n",
      "Accuracy on the train images: 72.5875%\n",
      "Epoch 5/30, Step [500/2000], Loss: 0.8390\n",
      "Epoch 5/30, Step [1000/2000], Loss: 0.7116\n",
      "Epoch 5/30, Step [1500/2000], Loss: 0.8518\n",
      "Epoch 5/30, Step [2000/2000], Loss: 0.3804\n",
      "Accuracy on the test images: 73.45%\n",
      "Accuracy on the train images: 73.5625%\n",
      "Epoch 6/30, Step [500/2000], Loss: 0.2563\n",
      "Epoch 6/30, Step [1000/2000], Loss: 0.1798\n",
      "Epoch 6/30, Step [1500/2000], Loss: 0.2161\n",
      "Epoch 6/30, Step [2000/2000], Loss: 0.3411\n",
      "Accuracy on the test images: 73.0%\n",
      "Accuracy on the train images: 72.925%\n",
      "Epoch 7/30, Step [500/2000], Loss: 0.3042\n",
      "Epoch 7/30, Step [1000/2000], Loss: 0.6402\n",
      "Epoch 7/30, Step [1500/2000], Loss: 0.4625\n",
      "Epoch 7/30, Step [2000/2000], Loss: 0.3513\n",
      "Accuracy on the test images: 74.9%\n",
      "Accuracy on the train images: 75.6375%\n",
      "Epoch 8/30, Step [500/2000], Loss: 0.3959\n",
      "Epoch 8/30, Step [1000/2000], Loss: 0.2918\n",
      "Epoch 8/30, Step [1500/2000], Loss: 0.4782\n",
      "Epoch 8/30, Step [2000/2000], Loss: 0.9759\n",
      "Accuracy on the test images: 76.1%\n",
      "Accuracy on the train images: 77.05%\n",
      "Epoch 9/30, Step [500/2000], Loss: 0.2716\n",
      "Epoch 9/30, Step [1000/2000], Loss: 0.2821\n",
      "Epoch 9/30, Step [1500/2000], Loss: 0.2993\n",
      "Epoch 9/30, Step [2000/2000], Loss: 0.4954\n",
      "Accuracy on the test images: 75.7%\n",
      "Accuracy on the train images: 77.6%\n",
      "Epoch 10/30, Step [500/2000], Loss: 0.5318\n",
      "Epoch 10/30, Step [1000/2000], Loss: 0.4108\n",
      "Epoch 10/30, Step [1500/2000], Loss: 0.4956\n",
      "Epoch 10/30, Step [2000/2000], Loss: 0.6830\n",
      "Accuracy on the test images: 73.7%\n",
      "Accuracy on the train images: 76.75%\n",
      "Epoch 11/30, Step [500/2000], Loss: 0.5151\n",
      "Epoch 11/30, Step [1000/2000], Loss: 0.4416\n",
      "Epoch 11/30, Step [1500/2000], Loss: 0.7223\n",
      "Epoch 11/30, Step [2000/2000], Loss: 1.2125\n",
      "Accuracy on the test images: 76.6%\n",
      "Accuracy on the train images: 78.4875%\n",
      "Epoch 12/30, Step [500/2000], Loss: 0.3106\n",
      "Epoch 12/30, Step [1000/2000], Loss: 0.1978\n",
      "Epoch 12/30, Step [1500/2000], Loss: 0.4793\n",
      "Epoch 12/30, Step [2000/2000], Loss: 0.2331\n",
      "Accuracy on the test images: 77.85%\n",
      "Accuracy on the train images: 80.05%\n",
      "Epoch 13/30, Step [500/2000], Loss: 1.3996\n",
      "Epoch 13/30, Step [1000/2000], Loss: 0.4354\n",
      "Epoch 13/30, Step [1500/2000], Loss: 0.7440\n",
      "Epoch 13/30, Step [2000/2000], Loss: 0.3000\n",
      "Accuracy on the test images: 77.35%\n",
      "Accuracy on the train images: 79.925%\n",
      "Epoch 14/30, Step [500/2000], Loss: 0.5564\n",
      "Epoch 14/30, Step [1000/2000], Loss: 0.5112\n",
      "Epoch 14/30, Step [1500/2000], Loss: 0.6038\n",
      "Epoch 14/30, Step [2000/2000], Loss: 0.9846\n",
      "Accuracy on the test images: 76.9%\n",
      "Accuracy on the train images: 80.775%\n",
      "Epoch 15/30, Step [500/2000], Loss: 0.0446\n",
      "Epoch 15/30, Step [1000/2000], Loss: 0.1356\n",
      "Epoch 15/30, Step [1500/2000], Loss: 0.5177\n",
      "Epoch 15/30, Step [2000/2000], Loss: 0.4102\n",
      "Accuracy on the test images: 75.65%\n",
      "Accuracy on the train images: 78.8625%\n",
      "Epoch 16/30, Step [500/2000], Loss: 0.3140\n",
      "Epoch 16/30, Step [1000/2000], Loss: 0.2244\n",
      "Epoch 16/30, Step [1500/2000], Loss: 0.5408\n",
      "Epoch 16/30, Step [2000/2000], Loss: 0.1216\n",
      "Accuracy on the test images: 77.55%\n",
      "Accuracy on the train images: 82.575%\n",
      "Epoch 17/30, Step [500/2000], Loss: 0.8994\n",
      "Epoch 17/30, Step [1000/2000], Loss: 0.2756\n",
      "Epoch 17/30, Step [1500/2000], Loss: 0.1637\n",
      "Epoch 17/30, Step [2000/2000], Loss: 0.6789\n",
      "Accuracy on the test images: 77.75%\n",
      "Accuracy on the train images: 81.8375%\n",
      "Epoch 18/30, Step [500/2000], Loss: 0.1868\n",
      "Epoch 18/30, Step [1000/2000], Loss: 0.2666\n",
      "Epoch 18/30, Step [1500/2000], Loss: 0.6760\n",
      "Epoch 18/30, Step [2000/2000], Loss: 0.4712\n",
      "Accuracy on the test images: 78.25%\n",
      "Accuracy on the train images: 82.175%\n",
      "Epoch 19/30, Step [500/2000], Loss: 0.2581\n",
      "Epoch 19/30, Step [1000/2000], Loss: 0.0451\n",
      "Epoch 19/30, Step [1500/2000], Loss: 0.7108\n",
      "Epoch 19/30, Step [2000/2000], Loss: 0.1159\n",
      "Accuracy on the test images: 77.55%\n",
      "Accuracy on the train images: 81.175%\n",
      "Epoch 20/30, Step [500/2000], Loss: 0.3088\n",
      "Epoch 20/30, Step [1000/2000], Loss: 0.6613\n",
      "Epoch 20/30, Step [1500/2000], Loss: 0.1774\n",
      "Epoch 20/30, Step [2000/2000], Loss: 0.1173\n",
      "Accuracy on the test images: 79.35%\n",
      "Accuracy on the train images: 83.65%\n",
      "Epoch 21/30, Step [500/2000], Loss: 0.4457\n",
      "Epoch 21/30, Step [1000/2000], Loss: 0.1818\n",
      "Epoch 21/30, Step [1500/2000], Loss: 0.3716\n",
      "Epoch 21/30, Step [2000/2000], Loss: 0.4338\n",
      "Accuracy on the test images: 78.3%\n",
      "Accuracy on the train images: 83.45%\n",
      "Epoch 22/30, Step [500/2000], Loss: 0.6579\n",
      "Epoch 22/30, Step [1000/2000], Loss: 0.7303\n",
      "Epoch 22/30, Step [1500/2000], Loss: 0.1396\n",
      "Epoch 22/30, Step [2000/2000], Loss: 0.8224\n",
      "Accuracy on the test images: 79.15%\n",
      "Accuracy on the train images: 83.8125%\n",
      "Epoch 23/30, Step [500/2000], Loss: 0.1108\n",
      "Epoch 23/30, Step [1000/2000], Loss: 0.1053\n",
      "Epoch 23/30, Step [1500/2000], Loss: 0.2892\n",
      "Epoch 23/30, Step [2000/2000], Loss: 0.3629\n",
      "Accuracy on the test images: 78.05%\n",
      "Accuracy on the train images: 84.35%\n",
      "Epoch 24/30, Step [500/2000], Loss: 1.1849\n",
      "Epoch 24/30, Step [1000/2000], Loss: 0.5258\n",
      "Epoch 24/30, Step [1500/2000], Loss: 0.2151\n",
      "Epoch 24/30, Step [2000/2000], Loss: 0.5289\n",
      "Accuracy on the test images: 80.05%\n",
      "Accuracy on the train images: 84.4625%\n",
      "Epoch 25/30, Step [500/2000], Loss: 0.1363\n",
      "Epoch 25/30, Step [1000/2000], Loss: 0.3942\n",
      "Epoch 25/30, Step [1500/2000], Loss: 0.5269\n",
      "Epoch 25/30, Step [2000/2000], Loss: 0.1475\n",
      "Accuracy on the test images: 78.75%\n",
      "Accuracy on the train images: 85.825%\n",
      "Epoch 26/30, Step [500/2000], Loss: 0.1549\n",
      "Epoch 26/30, Step [1000/2000], Loss: 0.3414\n",
      "Epoch 26/30, Step [1500/2000], Loss: 0.2230\n",
      "Epoch 26/30, Step [2000/2000], Loss: 0.1147\n",
      "Accuracy on the test images: 79.9%\n",
      "Accuracy on the train images: 86.1%\n",
      "Epoch 27/30, Step [500/2000], Loss: 0.4017\n",
      "Epoch 27/30, Step [1000/2000], Loss: 0.2020\n",
      "Epoch 27/30, Step [1500/2000], Loss: 2.0040\n",
      "Epoch 27/30, Step [2000/2000], Loss: 0.0030\n",
      "Accuracy on the test images: 79.2%\n",
      "Accuracy on the train images: 85.9375%\n",
      "Epoch 28/30, Step [500/2000], Loss: 0.5554\n",
      "Epoch 28/30, Step [1000/2000], Loss: 0.5229\n",
      "Epoch 28/30, Step [1500/2000], Loss: 0.1328\n",
      "Epoch 28/30, Step [2000/2000], Loss: 0.3971\n",
      "Accuracy on the test images: 78.25%\n",
      "Accuracy on the train images: 85.6125%\n",
      "Epoch 29/30, Step [500/2000], Loss: 0.0271\n",
      "Epoch 29/30, Step [1000/2000], Loss: 0.0291\n",
      "Epoch 29/30, Step [1500/2000], Loss: 0.3004\n",
      "Epoch 29/30, Step [2000/2000], Loss: 0.0973\n",
      "Accuracy on the test images: 80.35%\n",
      "Accuracy on the train images: 86.8625%\n",
      "Epoch 30/30, Step [500/2000], Loss: 0.3009\n",
      "Epoch 30/30, Step [1000/2000], Loss: 0.1747\n",
      "Epoch 30/30, Step [1500/2000], Loss: 0.3002\n",
      "Epoch 30/30, Step [2000/2000], Loss: 0.1632\n",
      "Accuracy on the test images: 80.15%\n",
      "Accuracy on the train images: 87.525%\n",
      "Training finished in 13859.351279497147 Seconds!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st=time.time()\n",
    "\n",
    "train_acc1=[]\n",
    "test_acc1=[]\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc1.append(100 * correct / total)\n",
    "    print(f'Accuracy on the test images: {100 * correct / total}%')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc1.append(100 * correct / total)\n",
    "    print(f'Accuracy on the train images: {100 * correct / total}%')\n",
    "\n",
    "    with open('Model4_test.pkl','wb') as file:\n",
    "            pickle.dump(test_acc1,file)\n",
    "    with open('Model4_train.pkl','wb') as file:\n",
    "            pickle.dump(train_acc1,file)\n",
    "    with open('Model4_test.pkl','rb') as file:\n",
    "            test_acc1=pickle.load(file)\n",
    "    with open('Model4_train.pkl','rb') as file:\n",
    "            train_acc1=pickle.load(file)\n",
    "\n",
    "ed=time.time()\n",
    "print('Training finished in '+str(ed-st)+' Seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69730054-a787-4d83-adec-967716a3260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.convres = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.pool(self.pool(self.pool(self.convres(x))))\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x += y\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf4bd19-87d0-4534-a8af-b10167fa942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),momentum=0.9,lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45fe05a-8d5d-452b-96ca-c484b47b151f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step [500/2000], Loss: 0.7633\n",
      "Epoch 1/30, Step [1000/2000], Loss: 0.8540\n",
      "Epoch 1/30, Step [1500/2000], Loss: 0.7273\n",
      "Epoch 1/30, Step [2000/2000], Loss: 0.6738\n",
      "Accuracy on the test images: 65.3%\n",
      "Accuracy on the train images: 64.9625%\n",
      "Epoch 2/30, Step [500/2000], Loss: 0.6766\n",
      "Epoch 2/30, Step [1000/2000], Loss: 0.4619\n",
      "Epoch 2/30, Step [1500/2000], Loss: 0.5125\n",
      "Epoch 2/30, Step [2000/2000], Loss: 0.7479\n",
      "Accuracy on the test images: 69.1%\n",
      "Accuracy on the train images: 70.175%\n",
      "Epoch 3/30, Step [500/2000], Loss: 0.3708\n",
      "Epoch 3/30, Step [1000/2000], Loss: 0.1834\n",
      "Epoch 3/30, Step [1500/2000], Loss: 0.8683\n",
      "Epoch 3/30, Step [2000/2000], Loss: 0.3774\n",
      "Accuracy on the test images: 67.45%\n",
      "Accuracy on the train images: 69.275%\n",
      "Epoch 4/30, Step [500/2000], Loss: 0.3179\n",
      "Epoch 4/30, Step [1000/2000], Loss: 0.5109\n",
      "Epoch 4/30, Step [1500/2000], Loss: 0.6973\n",
      "Epoch 4/30, Step [2000/2000], Loss: 0.7660\n",
      "Accuracy on the test images: 73.3%\n",
      "Accuracy on the train images: 72.875%\n",
      "Epoch 5/30, Step [500/2000], Loss: 0.4925\n",
      "Epoch 5/30, Step [1000/2000], Loss: 0.7565\n",
      "Epoch 5/30, Step [1500/2000], Loss: 0.4790\n",
      "Epoch 5/30, Step [2000/2000], Loss: 0.7020\n",
      "Accuracy on the test images: 74.65%\n",
      "Accuracy on the train images: 75.1%\n",
      "Epoch 6/30, Step [500/2000], Loss: 0.3256\n",
      "Epoch 6/30, Step [1000/2000], Loss: 0.4719\n",
      "Epoch 6/30, Step [1500/2000], Loss: 1.1188\n",
      "Epoch 6/30, Step [2000/2000], Loss: 0.3189\n",
      "Accuracy on the test images: 74.85%\n",
      "Accuracy on the train images: 76.125%\n",
      "Epoch 7/30, Step [500/2000], Loss: 0.4911\n",
      "Epoch 7/30, Step [1000/2000], Loss: 0.3627\n",
      "Epoch 7/30, Step [1500/2000], Loss: 0.1691\n",
      "Epoch 7/30, Step [2000/2000], Loss: 0.4674\n",
      "Accuracy on the test images: 75.5%\n",
      "Accuracy on the train images: 77.4375%\n",
      "Epoch 8/30, Step [500/2000], Loss: 0.5304\n",
      "Epoch 8/30, Step [1000/2000], Loss: 0.4603\n",
      "Epoch 8/30, Step [1500/2000], Loss: 0.9244\n",
      "Epoch 8/30, Step [2000/2000], Loss: 0.6232\n",
      "Accuracy on the test images: 75.05%\n",
      "Accuracy on the train images: 77.425%\n",
      "Epoch 9/30, Step [500/2000], Loss: 1.3113\n",
      "Epoch 9/30, Step [1000/2000], Loss: 0.3481\n",
      "Epoch 9/30, Step [1500/2000], Loss: 0.0883\n",
      "Epoch 9/30, Step [2000/2000], Loss: 0.6143\n",
      "Accuracy on the test images: 75.85%\n",
      "Accuracy on the train images: 77.8125%\n",
      "Epoch 10/30, Step [500/2000], Loss: 0.3240\n",
      "Epoch 10/30, Step [1000/2000], Loss: 0.7668\n",
      "Epoch 10/30, Step [1500/2000], Loss: 0.4726\n",
      "Epoch 10/30, Step [2000/2000], Loss: 0.7140\n",
      "Accuracy on the test images: 78.45%\n",
      "Accuracy on the train images: 80.55%\n",
      "Epoch 11/30, Step [500/2000], Loss: 0.2195\n",
      "Epoch 11/30, Step [1000/2000], Loss: 0.1932\n",
      "Epoch 11/30, Step [1500/2000], Loss: 0.9129\n",
      "Epoch 11/30, Step [2000/2000], Loss: 0.7705\n",
      "Accuracy on the test images: 74.8%\n",
      "Accuracy on the train images: 78.9375%\n",
      "Epoch 12/30, Step [500/2000], Loss: 0.3756\n",
      "Epoch 12/30, Step [1000/2000], Loss: 0.2344\n",
      "Epoch 12/30, Step [1500/2000], Loss: 0.4734\n",
      "Epoch 12/30, Step [2000/2000], Loss: 0.2648\n",
      "Accuracy on the test images: 77.05%\n",
      "Accuracy on the train images: 79.3%\n",
      "Epoch 13/30, Step [500/2000], Loss: 0.4549\n",
      "Epoch 13/30, Step [1000/2000], Loss: 0.1773\n",
      "Epoch 13/30, Step [1500/2000], Loss: 0.4821\n",
      "Epoch 13/30, Step [2000/2000], Loss: 0.1640\n",
      "Accuracy on the test images: 76.3%\n",
      "Accuracy on the train images: 79.8125%\n",
      "Epoch 14/30, Step [500/2000], Loss: 0.1360\n",
      "Epoch 14/30, Step [1000/2000], Loss: 0.5096\n",
      "Epoch 14/30, Step [1500/2000], Loss: 0.8428\n",
      "Epoch 14/30, Step [2000/2000], Loss: 1.0410\n",
      "Accuracy on the test images: 76.3%\n",
      "Accuracy on the train images: 79.675%\n",
      "Epoch 15/30, Step [500/2000], Loss: 0.3438\n",
      "Epoch 15/30, Step [1000/2000], Loss: 0.3883\n",
      "Epoch 15/30, Step [1500/2000], Loss: 0.4379\n",
      "Epoch 15/30, Step [2000/2000], Loss: 0.2958\n",
      "Accuracy on the test images: 77.6%\n",
      "Accuracy on the train images: 80.6625%\n",
      "Epoch 16/30, Step [500/2000], Loss: 0.1639\n",
      "Epoch 16/30, Step [1000/2000], Loss: 0.0928\n",
      "Epoch 16/30, Step [1500/2000], Loss: 0.5957\n",
      "Epoch 16/30, Step [2000/2000], Loss: 0.1734\n",
      "Accuracy on the test images: 79.45%\n",
      "Accuracy on the train images: 82.4875%\n",
      "Epoch 17/30, Step [500/2000], Loss: 0.0565\n",
      "Epoch 17/30, Step [1000/2000], Loss: 0.2706\n",
      "Epoch 17/30, Step [1500/2000], Loss: 0.4241\n",
      "Epoch 17/30, Step [2000/2000], Loss: 0.7105\n",
      "Accuracy on the test images: 77.05%\n",
      "Accuracy on the train images: 82.4625%\n",
      "Epoch 18/30, Step [500/2000], Loss: 0.4742\n",
      "Epoch 18/30, Step [1000/2000], Loss: 0.1988\n",
      "Epoch 18/30, Step [1500/2000], Loss: 0.3895\n",
      "Epoch 18/30, Step [2000/2000], Loss: 0.7120\n",
      "Accuracy on the test images: 77.6%\n",
      "Accuracy on the train images: 83.525%\n",
      "Epoch 19/30, Step [500/2000], Loss: 0.1807\n",
      "Epoch 19/30, Step [1000/2000], Loss: 0.7571\n",
      "Epoch 19/30, Step [1500/2000], Loss: 0.7612\n",
      "Epoch 19/30, Step [2000/2000], Loss: 0.1494\n",
      "Accuracy on the test images: 78.35%\n",
      "Accuracy on the train images: 82.9125%\n",
      "Epoch 20/30, Step [500/2000], Loss: 0.4026\n",
      "Epoch 20/30, Step [1000/2000], Loss: 0.1798\n",
      "Epoch 20/30, Step [1500/2000], Loss: 0.6491\n",
      "Epoch 20/30, Step [2000/2000], Loss: 0.0834\n",
      "Accuracy on the test images: 80.7%\n",
      "Accuracy on the train images: 84.475%\n",
      "Epoch 21/30, Step [500/2000], Loss: 0.1400\n",
      "Epoch 21/30, Step [1000/2000], Loss: 0.1772\n",
      "Epoch 21/30, Step [1500/2000], Loss: 0.3011\n",
      "Epoch 21/30, Step [2000/2000], Loss: 1.5590\n",
      "Accuracy on the test images: 78.8%\n",
      "Accuracy on the train images: 83.35%\n",
      "Epoch 22/30, Step [500/2000], Loss: 0.4002\n",
      "Epoch 22/30, Step [1000/2000], Loss: 0.4029\n",
      "Epoch 22/30, Step [1500/2000], Loss: 0.4422\n",
      "Epoch 22/30, Step [2000/2000], Loss: 0.1297\n",
      "Accuracy on the test images: 78.55%\n",
      "Accuracy on the train images: 83.5%\n",
      "Epoch 23/30, Step [500/2000], Loss: 0.1253\n",
      "Epoch 23/30, Step [1000/2000], Loss: 0.0838\n",
      "Epoch 23/30, Step [1500/2000], Loss: 0.5372\n",
      "Epoch 23/30, Step [2000/2000], Loss: 0.2348\n",
      "Accuracy on the test images: 78.85%\n",
      "Accuracy on the train images: 83.8625%\n",
      "Epoch 24/30, Step [500/2000], Loss: 0.1569\n",
      "Epoch 24/30, Step [1000/2000], Loss: 0.4841\n",
      "Epoch 24/30, Step [1500/2000], Loss: 0.3467\n",
      "Epoch 24/30, Step [2000/2000], Loss: 0.2866\n",
      "Accuracy on the test images: 79.2%\n",
      "Accuracy on the train images: 84.7%\n",
      "Epoch 25/30, Step [500/2000], Loss: 0.0368\n",
      "Epoch 25/30, Step [1000/2000], Loss: 1.1258\n",
      "Epoch 25/30, Step [1500/2000], Loss: 0.1986\n",
      "Epoch 25/30, Step [2000/2000], Loss: 0.2639\n",
      "Accuracy on the test images: 78.1%\n",
      "Accuracy on the train images: 85.325%\n",
      "Epoch 26/30, Step [500/2000], Loss: 0.1836\n",
      "Epoch 26/30, Step [1000/2000], Loss: 0.3169\n",
      "Epoch 26/30, Step [1500/2000], Loss: 0.7846\n",
      "Epoch 26/30, Step [2000/2000], Loss: 0.3229\n",
      "Accuracy on the test images: 78.9%\n",
      "Accuracy on the train images: 85.425%\n",
      "Epoch 27/30, Step [500/2000], Loss: 0.0342\n",
      "Epoch 27/30, Step [1000/2000], Loss: 0.1224\n",
      "Epoch 27/30, Step [1500/2000], Loss: 0.6005\n",
      "Epoch 27/30, Step [2000/2000], Loss: 0.0611\n",
      "Accuracy on the test images: 79.7%\n",
      "Accuracy on the train images: 86.625%\n",
      "Epoch 28/30, Step [500/2000], Loss: 0.2608\n",
      "Epoch 28/30, Step [1000/2000], Loss: 0.1783\n",
      "Epoch 28/30, Step [1500/2000], Loss: 0.4663\n",
      "Epoch 28/30, Step [2000/2000], Loss: 0.3586\n",
      "Accuracy on the test images: 79.65%\n",
      "Accuracy on the train images: 86.8625%\n",
      "Epoch 29/30, Step [500/2000], Loss: 0.1614\n",
      "Epoch 29/30, Step [1000/2000], Loss: 0.4295\n",
      "Epoch 29/30, Step [1500/2000], Loss: 0.5684\n",
      "Epoch 29/30, Step [2000/2000], Loss: 0.2526\n",
      "Accuracy on the test images: 80.95%\n",
      "Accuracy on the train images: 87.925%\n",
      "Epoch 30/30, Step [500/2000], Loss: 0.3672\n",
      "Epoch 30/30, Step [1000/2000], Loss: 1.2718\n",
      "Epoch 30/30, Step [1500/2000], Loss: 0.4962\n",
      "Epoch 30/30, Step [2000/2000], Loss: 0.9993\n",
      "Accuracy on the test images: 80.6%\n",
      "Accuracy on the train images: 87.875%\n",
      "Training finished in 14181.023787975311 Seconds!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st=time.time()\n",
    "\n",
    "train_acc2=[]\n",
    "test_acc2=[]\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc2.append(100 * correct / total)\n",
    "    print(f'Accuracy on the test images: {100 * correct / total}%')\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in train_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_acc2.append(100 * correct / total)\n",
    "    print(f'Accuracy on the train images: {100 * correct / total}%')\n",
    "    \n",
    "    with open('Model5_test.pkl','wb') as file:\n",
    "            pickle.dump(test_acc2,file)\n",
    "    with open('Model5_train.pkl','wb') as file:\n",
    "            pickle.dump(train_acc2,file)\n",
    "    with open('Model5_test.pkl','rb') as file:\n",
    "            test_acc2=pickle.load(file)\n",
    "    with open('Model5_train.pkl','rb') as file:\n",
    "            train_acc2=pickle.load(file)\n",
    "\n",
    "ed=time.time()\n",
    "print('Training finished in '+str(ed-st)+' Seconds!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
